{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model for an online book selling website"
      ],
      "metadata": {
        "id": "NWSd5Rssa7rM"
      },
      "id": "NWSd5Rssa7rM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has been trained only on two books for demonstration but can be trained on many more, if we want.\n",
        "These two books are Rich Dad and Poor Dad, The Power of Your Subconscious Mind."
      ],
      "metadata": {
        "id": "7qHGLhG_bGsa"
      },
      "id": "7qHGLhG_bGsa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746493b1",
      "metadata": {
        "id": "746493b1",
        "outputId": "50300ec1-4622-46db-affa-37dda93b5316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8 images belonging to 2 classes.\n",
            "Found 2 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.6713 - accuracy: 0.5000 - val_loss: 0.7465 - val_accuracy: 0.5000\n",
            "Epoch 2/25\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7002 - accuracy: 0.5000 - val_loss: 0.6442 - val_accuracy: 1.0000\n",
            "Epoch 3/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.5720 - accuracy: 0.8750 - val_loss: 0.6483 - val_accuracy: 0.5000\n",
            "Epoch 4/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.5265 - accuracy: 0.8750 - val_loss: 0.6090 - val_accuracy: 0.5000\n",
            "Epoch 5/25\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.4297 - accuracy: 0.8750 - val_loss: 0.6249 - val_accuracy: 0.5000\n",
            "Epoch 6/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.3868 - accuracy: 0.7500 - val_loss: 0.6018 - val_accuracy: 0.5000\n",
            "Epoch 7/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3211 - accuracy: 0.8750 - val_loss: 0.5590 - val_accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2705 - accuracy: 0.8750 - val_loss: 0.5719 - val_accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2270 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2119 - accuracy: 0.8750 - val_loss: 0.4704 - val_accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1571 - accuracy: 0.8750 - val_loss: 0.4375 - val_accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1211 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0909 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0586 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 0.3584 - val_accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.3026 - val_accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.2857 - val_accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2748 - val_accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.2509 - val_accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2443 - val_accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2311 - val_accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 9.7343e-04 - accuracy: 1.0000 - val_loss: 0.1812 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "RichDadPoorDad\n"
          ]
        }
      ],
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "# Importing the libraries\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "tf.__version__\n",
        "\n",
        "# Part 1 - Data Preprocessing\n",
        "\n",
        "# Preprocessing the Training set\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('Train_set',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "# Preprocessing the Test set\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('Test_set',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# Part 2 - Building the CNN\n",
        "\n",
        "# Initialising the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Step 1 - Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
        "\n",
        "# Step 2 - Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Adding a second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Step 3 - Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Step 4 - Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Step 5 - Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Part 3 - Training the CNN\n",
        "\n",
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Training the CNN on the Training set and evaluating it on the Test set\n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
        "\n",
        "# Part 4 - Making a single prediction\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_image = image.load_img('new_test_image.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices\n",
        "if result[0][0] == 1:\n",
        "    prediction = 'RichDadPoorDad'\n",
        "else:\n",
        "    prediction = 'PowerOfYourSubconsciousMind'\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fa8cad",
      "metadata": {
        "id": "15fa8cad"
      },
      "outputs": [],
      "source": [
        "if prediction=='RichDadPoorDad':\n",
        "    refFilename = 'base_image_rich.jpg'\n",
        "else:\n",
        "    refFilename = 'base_image_power.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Alignment using ORB and RANSAC,\n",
        "sift or surf can also be used in place of ORB in a similar manner."
      ],
      "metadata": {
        "id": "5TXSfPvJboDs"
      },
      "id": "5TXSfPvJboDs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image containing the matches and final image after alignment are created below and are present in the Folder containing this file as well"
      ],
      "metadata": {
        "id": "zs-bixICcDs_"
      },
      "id": "zs-bixICcDs_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faff1d4c",
      "metadata": {
        "id": "faff1d4c",
        "outputId": "dcce0d71-ecd2-4175-fc70-e732d23ad771"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading reference image :  base_image_rich.jpg\n",
            "Aligning images ...\n",
            "Saving aligned image :  aligned.jpg\n",
            "Estimated homography : \n",
            " [[ 7.37782366e-01 -7.06815630e-01  1.59947729e+02]\n",
            " [ 4.49344536e-01  7.49280232e-01  4.46194136e+01]\n",
            " [-3.55399201e-04 -3.72163188e-04  1.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "MAX_FEATURES = 500\n",
        "GOOD_MATCH_PERCENT = 0.15\n",
        "\n",
        "def alignImages(im1, im2):\n",
        "\n",
        "  # Convert images to grayscale\n",
        "  im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
        "  im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Detect ORB features and compute descriptors.\n",
        "  orb = cv2.ORB_create(MAX_FEATURES)\n",
        "  keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
        "  keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
        "\n",
        "  # Match features.\n",
        "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
        "  matches = matcher.match(descriptors1, descriptors2, None)\n",
        "\n",
        "  # Sort matches by score\n",
        "  matches = sorted(matches,key=lambda x: x.distance, reverse=False)\n",
        "\n",
        "  # Remove not so good matches\n",
        "  numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
        "  matches = matches[:numGoodMatches]\n",
        "\n",
        "  # Draw top matches\n",
        "  imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
        "  cv2.imwrite(\"matches.jpg\", imMatches)\n",
        "\n",
        "  # Extract location of good matches\n",
        "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
        "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
        "\n",
        "  for i, match in enumerate(matches):\n",
        "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
        "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
        "\n",
        "  # Find homography\n",
        "  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
        "\n",
        "  # Use homography\n",
        "  height, width, channels = im2.shape\n",
        "  im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
        "\n",
        "  return im1Reg, h\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # Read reference image\n",
        "  print(\"Reading reference image : \", refFilename)\n",
        "  imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
        "\n",
        "  # Read image to be aligned\n",
        "\n",
        "  print(\"Aligning images ...\")\n",
        "  # Registered image will be resotred in imReg.\n",
        "  # The estimated homography will be stored in h.\n",
        "  imReg, h = alignImages(cv2.imread('new_test_image.jpg', cv2.IMREAD_COLOR), imReference)\n",
        "\n",
        "  # Write aligned image to disk.\n",
        "  outFilename = \"aligned.jpg\"\n",
        "  print(\"Saving aligned image : \", outFilename);\n",
        "  cv2.imwrite(outFilename, imReg)\n",
        "\n",
        "  # Print estimated homography\n",
        "  print(\"Estimated homography : \\n\",  h)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**:- Image Alignment does not function that efficiently as that crops and resize the image based upon the features, and that depends on our test image based on which may be we can get very good result and may be we can get not so satisfying result."
      ],
      "metadata": {
        "id": "AYv180UhcbFE"
      },
      "id": "AYv180UhcbFE"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}